
---
title: "Data Ethics: Algorithmic Privacy and Scarce Resource Allocation"
subtitle: "CDTO Campus 2024: *Shaping the Digital Future*"
author: "Jeff Jacobs"
institute: "Georgetown University<br /><a href='mailto:jj1088@georgetown.edu'>`jj1088@georgetown.edu`</a>"
date: 2024-08-08
date-format: "dddd, D MMMM YYYY"
format:
  revealjs:
    output-file: "index.html"
    include-in-header:
      text: |
        <link rel="stylesheet" href="https://unpkg.com/leaflet@1.9.4/dist/leaflet.css" integrity="sha256-p4NxAoJBhIIN+hmNHrzRCf9tD/miZyoHS5obTRR9BMY=" crossorigin=""/>
        <script src="https://unpkg.com/leaflet@1.9.4/dist/leaflet.js" integrity="sha256-20nQCchB9co0qIjJZRGuk2/Z9VM+kNiyxNV1lvTlZBo=" crossorigin=""></script>
        <script src="https://unpkg.com/leaflet.gridlayer.googlemutant@latest/dist/Leaflet.GoogleMutant.js"></script>
    favicon: "favicon.png"
    html-math-method: mathjax
    cache: true
    slide-number: true
    scrollable: true
    link-external-icon: true
    link-external-newwindow: true
    theme: [default, 'jjslides.scss']
    footer: "Shaping the Digital Future: Data Ethics"
    logo: "images/DSAN_Combined.png"
    bibliography: "cdto.bib"
    nocite: |
      @*
    revealjs-plugins:
      - simplemenu
    simplemenu:
      flat: true
      barhtml:
        header: "<div class='menubar'><ul class='menu'></ul></div>"
      scale: 0.5
---

# <mark data-term="–ü—Ä–∏–≤—ñ—Ç!">Hello!</mark> {data-stack-name="–ü—Ä–∏–≤—ñ—Ç!"}

## My Goal {.entities}

* I want to provide some <mark data-term="—ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏">tools</mark> from <mark data-term="–µ—Ç–∏—á–Ω–µ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –¥–∞–Ω–∏–º–∏">data ethics</mark>, for your <mark data-term="—è—â–∏–∫ –¥–ª—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤">toolbox</mark>
* If one of the tools is not relevant, I hope another will be!

## Who Am I? How Can I Help You?

```{=html}
<table>
<thead>
<tr>
  <th align="center">üíª</th>
  <th></th>
  <th align="center">üåè</th>
</tr>
</thead>
<tbody>
<tr>
  <td><span data-qmd="MS in **Computer Science**, Stanford University"></span></td>
  <td>&rarr;</td>
  <td><span data-qmd="PhD in **International Relations**, Columbia University"></span></td>
</tr>
<tr>
  <td><span data-qmd="Assistant Teaching Professor, **Data Science and Analytics**"></span></td>
  <td>&rarr;</td>
  <td><span data-qmd="Courtesy Teaching Professor, **McCourt School of Public Policy**"></span></td>
</tr>
</tbody>
</table>
```

* Cross-listed course: [Data Ethics and Policy](https://jjacobs.me/dsan5450/){target='_blank'} (All slides/lectures available at that link!)

## Dissertation: "Our Word is Our Weapon" {.smaller .crunch-img .title-12}

* Cold War arms shipments (SIPRI) vs. propaganda (*–ü–µ—á–∞—Ç—å –°–°–°–†*): here, to üá™üáπ

![](images/eth_arms.svg){fig-align="center"}

* I only know the most basic <mark data-term="—É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ —Å–ª–æ–≤–∞">words</mark> üôà but I will try my best!

## Data Ethics in Times of War {.smaller .crunch-title .crunch-img .crunch-th .crunch-td-last}

```{=html}
<table>
<colgroup>
  <col span="1" style="width: 50%;">
  <col span="1" style="width: 50%;">
</colgroup>
<thead>
<tr>
  <th align="center">üáµüá∏</th>
  <th align="center">üá∫üá¶</th>
</tr>
</thead>
<tbody>
<tr>
  <td style="border-bottom: 0px !important;"><span data-qmd="2015-2023: Taught **Mobile App Development** in **West Bank**+**Gaza**"></span></td>
  <td style="border-bottom: 0px !important;"><span data-qmd="So when I learned of **–¥—ñ—è**, I thought some lessons/skills could apply!"></span></td>
</tr>
<tr>
  <td align="center" style="vertical-align: top;"><img src='images/doroob_crop.png' width="80%"></td>
  <td align="center" style="vertical-align: top;"><img src='images/diia.png' width="78%"></td>
</tr>
</tbody>
</table>
```

# Ethical Frameworks: Implicit and Explicit {data-stack-name="Ethical Frameworks"}

* We are always using <mark data-term="–µ—Ç–∏—á–Ω–∞ —Ç–µ–æ—Ä—ñ—è">**ethical frameworks**</mark>, even when we don't realize it!

## Cake Cutting

* You and a friend are very <mark data-term="–≥–æ–ª–æ–¥–Ω—ñ.">hungry</mark>, and you come across a <mark data-term="—Å–º–∞—á–Ω–∏–π —Ç–æ—Ä—Ç">tasty **cake**</mark>!
* How do you divide it?

## Ethical Frameworks

* Your instinct may be: divide <mark data-term="–ø–æ—Ä—ñ–≤–Ω—É">**equally**</mark>, so that you and your friend are treated equally
* But... are you treated equally?
* What if your friend is 10 times **hungrier**?
* What if your stomach is sensitive to sugar, so you can't eat more than $1/4$?

## Ethical Framework Implicit in Learning "Who You Are"! {.smaller .crunch-title .crunch-quarto-figure}

::: {.columns}
::: {.column width="50%"}

```{python}
#| label: orig-wordcloud
#| fig-align: center
import spacy
nlp = {
  'en': spacy.load("en_core_web_sm"),
  'uk': spacy.load("uk_core_news_sm")
}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
portfolio_df = pd.read_csv("Student_portfolios.csv")
portfolio_df = portfolio_df[~pd.isna(portfolio_df['Bio'])].copy()
bio_list_en = list(portfolio_df['Bio'].values)
# Preprocess bios individually
def clean_bio(raw_bio_str, lang):
  lang_nlp = nlp[lang]
  bio_doc = lang_nlp(raw_bio_str)
  bio_tokens = [t.text for t in bio_doc if not t.is_stop]
  bio_sent = " ".join(bio_tokens)
  return bio_sent
def clean_bios(raw_bio_list, lang):
  cleaned_sents = []
  for cur_bio in raw_bio_list:
    bio_sent = clean_bio(cur_bio, lang)
    cleaned_sents.append(bio_sent)
  return cleaned_sents
cleaned_bio_list_en = clean_bios(bio_list_en, 'en')
def combine_bios(bio_list):
  bio_str = "\n".join(bio_list)
  return bio_str
bio_str_en = combine_bios(cleaned_bio_list_en)
wc_options = {
  'background_color': 'white',
  'font_path': 'assets/Roboto.ttf',
  'scale': 8,
  'random_state': 2024,
  'width': 500,
  'height': 400,
}
def display_wordcloud(text_str):
  cloud = WordCloud(**wc_options)
  cloud.generate(text_str)
  plt.imshow(cloud, interpolation='bilinear')
  plt.axis("off")
display_wordcloud(bio_str_en)
```

:::
::: {.column width="50%"}

```{python}
#| label: orig-wordcloud-uk
#| fig-align: center
bio_list_uk = list(portfolio_df['Bio_uk'].values)
cleaned_bio_list_uk = clean_bios(bio_list_uk, 'uk')
bio_str_uk = combine_bios(cleaned_bio_list_uk)
display_wordcloud(bio_str_uk)
```

:::
:::

* How might this be **biased**, relative to **equal** contribution from everyone?

## Now With Fake Jeff Bio {.crunch-title .crunch-img}

::: {.columns}
::: {.column width="50%"}

```{python}
#| label: jeff-bio-en
jj_sent_en = "Jeff teaches Data Ethics at Georgetown University."
jj_bio_en = " ".join([jj_sent_en] * 100)
print(jj_bio_en[:230] + "...")
```

```{python}
#| label: wordcloud-jeff-bio-en
#| fig-align: center
bio_list_fake_en = cleaned_bio_list_en.copy()
cleaned_jj_bio_en = clean_bio(jj_bio_en, 'en')
bio_list_fake_en.append(cleaned_jj_bio_en)
bio_str_fake_en = combine_bios(bio_list_fake_en)
bio_cloud_fake_en = WordCloud(**wc_options)
bio_cloud_fake_en.generate(bio_str_fake_en)
plt.imshow(bio_cloud_fake_en, interpolation='bilinear')
plt.axis("off")
```

:::
::: {.column width="50%"}

```{python}
#| label: jeff-bio-uk
jj_sent_uk = "–î–∂–µ—Ñ –≤–∏–∫–ª–∞–¥–∞—î –µ—Ç–∏–∫—É –¥–∞–Ω–∏—Ö —É –î–∂–æ—Ä–¥–∂—Ç–∞—É–Ω—Å—å–∫–æ–º—É —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—ñ."
jj_bio_uk = " ".join([jj_sent_uk] * 100)
print(jj_bio_uk[:200] + "...")
```

```{python}
#| label: wordcloud-jeff-bio-uk
#| fig-align: center
bio_list_fake_uk = cleaned_bio_list_uk.copy()
cleaned_jj_bio_uk = clean_bio(jj_bio_uk, 'uk')
bio_list_fake_uk.append(cleaned_jj_bio_uk)
bio_str_fake_uk = combine_bios(bio_list_fake_uk)
bio_cloud_fake_uk = WordCloud(**wc_options)
bio_cloud_fake_uk.generate(bio_str_fake_uk)
plt.imshow(bio_cloud_fake_uk, interpolation='bilinear')
plt.axis("off")
```

:::
:::

## Normalizing Counts {.crunch-title .crunch-img}

* This is where **ethical framework** comes into play, whether implicit or explicit!
* Normalized: each person $i$ gets weight $\omega_i = 1/N$

::: {.columns}
::: {.column width="50%"}

```{python}
#| label: normalize-freqs-en
# Each bio's freqs should count 1/N in overall
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import normalize
cv_en = CountVectorizer(
  min_df=1,
  # stop_words='english'
)
dtm_en_sparse = cv_en.fit_transform(bio_list_fake_en)
dtm_en = dtm_en_sparse.todense()
#print(dtm.shape)
row_sums_en = dtm_en.sum(axis=1)
#print(row_sums.shape)
dtm_r0_en = dtm_en[0,:]
r0_sum_en = dtm_r0_en.sum()
r0_norm_en = dtm_r0_en / r0_sum_en
# Now use `normalize()`
dtm_norm_en = normalize(np.asarray(dtm_en), axis=1, norm='l1')
# And compute col sums of normalized DTM
dtm_colsums_en = dtm_norm_en.sum(axis=0)
# And combine with the words in a df
freq_df_en = pd.DataFrame({
  'term': cv_en.get_feature_names_out(),
  'freq': dtm_colsums_en
})
#freq_df['term'].values
```

```{python}
#| label: normalized-wordcloud-en
#| fig-align: center
freq_dict_en = freq_df_en.set_index('term').to_dict(orient='dict')['freq']
bio_cloud_eq_en = WordCloud(**wc_options)
bio_cloud_eq_en.generate_from_frequencies(freq_dict_en)
plt.imshow(bio_cloud_eq_en, interpolation='bilinear')
plt.axis("off")
```

:::
::: {.column width="50%"}

```{python}
#| label: normalize-freqs-uk
cv_uk = CountVectorizer(
  min_df=1,
)
dtm_uk_sparse = cv_uk.fit_transform(bio_list_fake_uk)
dtm_uk = dtm_uk_sparse.todense()
#print(dtm.shape)
row_sums_uk = dtm_uk.sum(axis=1)
#print(row_sums.shape)
dtm_r0_uk = dtm_uk[0,:]
r0_sum_uk = dtm_r0_uk.sum()
r0_norm_uk = dtm_r0_uk / r0_sum_uk
# Now use `normalize()`
dtm_norm_uk = normalize(np.asarray(dtm_uk), axis=1, norm='l1')
# And compute col sums of normalized DTM
dtm_colsums_uk = dtm_norm_uk.sum(axis=0)
# And combine with the words in a df
freq_df_uk = pd.DataFrame({
  'term': cv_uk.get_feature_names_out(),
  'freq': dtm_colsums_uk
})
#freq_df['term'].values
```

```{python}
#| label: normalized-wordcloud-uk
#| fig-align: center
freq_dict_uk = freq_df_uk.set_index('term').to_dict(orient='dict')['freq']
bio_cloud_eq_uk = WordCloud(**wc_options)
bio_cloud_eq_uk.generate_from_frequencies(freq_dict_uk)
plt.imshow(bio_cloud_eq_uk, interpolation='bilinear')
plt.axis("off")
```

:::
:::

## Equality of What? / –†—ñ–≤–Ω—ñ—Å—Ç—å... –ß–æ–≥–æ? {.crunch-title .crunch-ul .math-80}

* Is $\omega_i = 1/N$ the "correct" choice?
* Should we ensure **each person/ministry gets $\geq 1$ big word**
* Maybe it should be 50% **public sector**, 50% **private sector**
* **Higher $\omega$** for those **sitting closer to me**: $\omega_i = \frac{1}{\text{dist}(\text{Jeff},i)}$
* ...How to decide? **Objective Function** $f$ + **Constraints** $g$!

::: {.columns}
::: {.column width="50%"}

<center style="font-size: 80%;">
In General:
</center>

$$
\begin{align*}
\max_{c \mkern1.0mu \in \mkern1.0mu \text{Choices}} \; & f(c) = \text{Goodness of }c \\
\text{subject to } & g(c) = \text{Constraints on }c
\end{align*}
$$

:::
::: {.column width="50%"}

<center style="font-size: 80%;">
Our Example:
</center>

$$
\begin{align*}
\max_{\omega \mkern1.0mu \in \mkern1.0mu [0,1]^N} \; & f(\omega) = \textstyle\sum_{i=0}^{N}u_i(\omega) \\
\text{subject to } &\textstyle\sum_{i=0}^{N}\omega_i = 1
\end{align*}
$$

:::
:::

# Resource Allocation {data-stack-name="Resource Allocation"}

## Background: Indochina Mine Defusal {.crunch-title}

::: {.columns}
::: {.column width="50%"}

* Using **GIS** to allocate **mine defusal teams** to regions of Indochina mined by the US
* What information do we need about each region?
  * Absolute population? Population density? Mines per person?
  * Can equipment be transported there?

:::
::: {.column width="50%"}

![](images/indochina.png){fig-align="center" width="90%"}

:::
:::

## Now in –£–∫—Ä–∞—ó–Ω–∞

```{=html}
<style>
.embed-container {position: relative; padding-bottom: 67%; height: 0; max-width: 100%;} .embed-container iframe, .embed-container object, .embed-container iframe{position: absolute; top: 0; left: 0; width: 100%; height: 100%;} small{position: absolute; z-index: 40; bottom: 0; margin-bottom: -15px;}
</style>
<center>
<div class="embed-container">
  <iframe width="600" height="350" frameborder="0" scrolling="no" marginheight="0" marginwidth="0" title="Ukraine_Regions" src="//georgetownuniv.maps.arcgis.com/apps/Embed/index.html?webmap=50668b6cb4664b79948817100400b0dc&extent=19.2882,43.8227,44.7765,53.1106&home=true&zoom=true&previewImage=false&scale=true&disable_scroll=true&theme=light"></iframe>
</div>
</center>
```

## Incremental Model

* As a **Minimum Viable Product (MVP)**, first just consider the **non-constrained** optimization
* Add in constraints incrementally---as you add, evaluate **feasibility** relative to **goals** (for example, how close can you get to MVP outcome?)
* Iterative process $\implies$ **visual dashboards** can be crucially helpful!

## Interactive Example

* [ObservableJS Interactive Dashboard](https://observablehq.com/@jpowerj/resource-allocation-ukraine){target='_blank'}

## Problem 1: Divisibility {.crunch-title}

::: {.columns}
::: {.column width="50%"}

* Can we infinitely **split** our resources?
* $1/2$ of a gas mask doesn't help us! ‚ò†Ô∏è

:::
::: {.column width="50%"}

![](images/tennis_lifehack.jpeg){fig-align="center"}

:::
:::

## Problem 2: Distance Metrics {.crunch-title}

::: {.columns}
::: {.column width="50%"}

* Euclidean / "Straight line" distance **not always the "right" measure!**

:::
::: {.column width="50%"}

![From @shahid_comparison_2009](images/medical_distances.jpg){fig-align="center" width="75%"}

:::
:::

## Problem 3: Capabilities {.crunch-title}

::: {.columns}
::: {.column width="50%"}

* How effectively can people **"convert"** *resource* $\leadsto$ *utility*?

:::
::: {.column width="50%"}

![@sen_commodities_1985](images/sen_capabilities.jpg){fig-align="center" width="65%"}

:::
:::

# Privacy-Preserving Computation {data-stack-name="Privacy"}

## Disclosure Avoidance {.crunch-title .crunch-ul .crunch-p}

* 1990, 2000, and 2010 Census: "Data Swapping":

> We inject "noise" into the data by swapping records for certain households with those from households with similar characteristics in a nearby area. The Census does not release information about its specific methods for swapping. While this <mark data-term="–∫–æ–Ω—Ñ—ñ–¥–µ–Ω—Ü—ñ–π–Ω—ñ—Å—Ç—å">confidentiality</mark> around swapping techniques is important to protect against disclosure, it means that the practice is <mark data-term="–Ω–µ —î –ø—Ä–æ–∑–æ—Ä–∏–º">not transparent</mark> to data users

## The Death of Data Swapping

* But... machine learning, neural networks, etc...

> If traditional disclosure avoidance techniques were applied to the 2020 Census data, the amount of noise required to protect against new attacks would make census data unfit for most uses

## Differential Privacy

* Implemented for the [**2020 US Census**](https://www2.census.gov/library/publications/decennial/2020/2020-census-disclosure-avoidance-handbook.pdf){target='_blank'}
* The key idea: as people make more and more **queries** to your DB, your **response** gets less and less accurate

## Real Data: —Å–µ–ª–∏—â–µ –ü–æ–Ω–æ—Ä–Ω–∏—Ü—è

```{=html}
<div id='ponornycka' style="height: 400px;"></div>
<script src="assets/ponornycka.js"></script>
```

## Example with Real Data {.smaller}

```{=html}
<style>
/* Table section sep */
.tss td:nth-child(1), .tss td:nth-child(4), .tss td:nth-child(7), .tss td:nth-child(10) {
  /* background-color: green; */
  border-right: 2px solid darkgrey;
}
</style>
<table class='text-90 tss'>
<thead>
<tr>
  <th rowspan="2" align="center" style="vertical-align: middle;" class="trb">–≥—Ä–æ–º–∞–¥–∞</th>
  <th colspan="3" align="center" class="trb">True Population</th>
  <th colspan="3" align="center" class="trb">Noise</th>
  <th colspan="3" align="center" class="trb">Noisy Counts</th>
  <th colspan="3" align="center">Post-Processed</th>
</tr>
<tr>
  <th>&lt;18</th>
  <th>18+</th>
  <th class="trb">Total</th>
  <th>&lt;18</th>
  <th>18+</th>
  <th class="trb">Total</th>
  <th>&lt;18</th>
  <th>18+</th>
  <th class="trb">Total</th>
  <th>&lt;18</th>
  <th>18+</th>
  <th>Total</th>
</tr>
</thead>
<tbody>
<tr>
  <td>–ó–µ–ª–µ–Ω–∞ –ü–æ–ª—è–Ω–∞</td>
  <td align="right">20</td>
  <td align="right">57</td>
  <td align="right">77</td>
  <td align='right'><span class='pm-neutral'>0</span></td>
  <td align='right'><span class='pm-subtract'>-4</span></td>
  <td align='right'><span class='pm-add'>+2</span></td>
  <td align='right'>20</td>
  <td align='right'>53</td>
  <td align='right'>(&ne;)79</td>
  <td align='right'>23 (+3)</td>
  <td align='right'>54 (+1)</td>
  <td align='right'>77 (-2)</td>
</tr>
<tr>
  <td>–†–∏—Ö–ª–∏</td>
  <td>30</td>
  <td>82</td>
  <td>112</td>
  <td><span class='pm-subtract'>-3</span></td>
  <td><span class='pm-add'>+2</span></td>
  <td><span class='pm-add'>+3</span></td>
  <td>27</td>
  <td>84</td>
  <td>115</td>
  <td>28 (+1)</td>
  <td>85 (+1)</td>
  <td>113 (-2)</td>
</tr>
<tr>
  <td>–í–µ–ª–∏–∫–∏–π –õ—ñ—Å</td>
  <td>20</td>
  <td>86</td>
  <td>106</td>
  <td><span class='pm-subtract'>-2</span></td>
  <td><span class='pm-add'>+1</span></td>
  <td><span class='pm-add'>+1</span></td>
  <td>18</td>
  <td>87</td>
  <td>107</td>
  <td>17 (-1)</td>
  <td>87</td>
  <td>104 (-3)</td>
</tr>
<tr>
  <td>–†–æ–∑–ª—å–æ—Ç–∏</td>
  <td>150</td>
  <td>155</td>
  <td>305</td>
  <td><span class='pm-neutral'>0</span></td>
  <td><span class='pm-add'>+2</span></td>
  <td><span class='pm-neutral'>0</span></td>
  <td>150</td>
  <td>157</td>
  <td>305</td>
  <td>150</td>
  <td>156 (-1)</td>
  <td>306 (+1)</td>
</tr>
<tr>
  <td align="right"><b>Total</b></td>
  <td>220</td>
  <td>380</td>
  <td>600</td>
  <td><span class='pm-subtract'>-5</span></td>
  <td><span class='pm-add'>+1</span></td>
  <td><span class='pm-add'>+6</span></td>
  <td>215</td>
  <td>381</td>
  <td>606</td>
  <td>218</td>
  <td>382</td>
  <td>600</td>
</tr>
</tbody>
</table>
```

## The Magic (Noise!) {.crunch-title .crunch-ul .crunch-math .crunch-p .math-80}

* If noise values $\nu$ drawn i.i.d. from **Laplace distribution** (pdf):

$$
\text{Lap}(x; \varepsilon) = \frac{1}{2\varepsilon}\exp\mkern-2.5mu\left[-\frac{|x|}{\varepsilon}\right]
$$

* Can **mathematically guarantee** $\varepsilon$-differential privacy!

$$
\Pr\left[M(D) \in S\right] \leq e^\varepsilon \times \Pr\mkern-2.5mu\left[M(D') \in S\right]
$$

* Can **quantify** amount of privacy loss $\mathcal{L}(\gamma)$ from observing $\gamma$

$$
\mathcal{L}(\gamma) = \ln\left[\frac{\Pr[M(D) = \gamma]}{\Pr[M (D') = \gamma}\right]
$$

## The Metaphor

![Seurat, *A Sunday Afternoon on the Island of La Grande Jatte*, [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:A_Sunday_on_La_Grande_Jatte,_Georges_Seurat,_1884.jpg){target='_blank'}](images/seurat.jpg){fig-align="center"}

## References / Resources

::: {#refs}
:::

<!-- * Privacy: [Big Data and the Public Good](https://www.cambridge.org/core/books/privacy-big-data-and-the-public-good/1ACB10292B07EC30F071B4AD9650955C){target='_blank'} -->

# Q & A

# Thank You!
