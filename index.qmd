
---
title: "Data Ethics: Algorithmic Privacy and Scarce Resource Allocation"
subtitle: "CDTO Campus 2024: *Shaping the Digital Future*"
author: "Jeff Jacobs"
institute: "Georgetown University<br /><a href='mailto:jj1088@georgetown.edu'>`jj1088@georgetown.edu`</a>"
date: 2024-08-08
date-format: "dddd, D MMMM YYYY"
format:
  revealjs:
    output-file: "index.html"
    include-in-header:
      text: |
        <link rel="stylesheet" href="https://unpkg.com/leaflet@1.9.4/dist/leaflet.css" integrity="sha256-p4NxAoJBhIIN+hmNHrzRCf9tD/miZyoHS5obTRR9BMY=" crossorigin=""/>
        <script src="https://unpkg.com/leaflet@1.9.4/dist/leaflet.js" integrity="sha256-20nQCchB9co0qIjJZRGuk2/Z9VM+kNiyxNV1lvTlZBo=" crossorigin=""></script>
        <script src="https://unpkg.com/leaflet.gridlayer.googlemutant@latest/dist/Leaflet.GoogleMutant.js"></script>
    favicon: "favicon.png"
    html-math-method: mathjax
    cache: true
    slide-number: true
    scrollable: true
    link-external-icon: true
    link-external-newwindow: true
    theme: [default, 'jjslides.scss']
    footer: "Shaping the Digital Future: Data Ethics"
    logo: "images/DSAN_Combined.png"
    bibliography: "cdto.bib"
    nocite: |
      @*
    revealjs-plugins:
      - simplemenu
    simplemenu:
      flat: true
      barhtml:
        header: "<div class='menubar'><ul class='menu'></ul></div>"
      scale: 0.5
---

# <mark data-term="–ü—Ä–∏–≤—ñ—Ç!">Hello!</mark> {data-stack-name="–ü—Ä–∏–≤—ñ—Ç!"}

## My Goal {.entities}

* I want to provide some <mark data-term="—ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏">tools</mark> from <mark data-term="–µ—Ç–∏—á–Ω–µ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –¥–∞–Ω–∏–º–∏">data ethics</mark>, for your <mark data-term="—è—â–∏–∫ –¥–ª—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤">toolbox</mark>
* If one of the tools is <mark data-term="–Ω–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ">not relevant</mark>, I hope another will be!
* If one of the tools is <mark data-term="–¥—É–∂–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ">very relevant</mark>, I can provide more resources (books, articles) for you!

::: {.notes}

–µ—Ç–∏—á–Ω–µ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –¥–∞–Ω–∏–º–∏ = etychne upravlinnya danymy = ethical data management

:::

## Who Am I? How Can I Help You?

```{=html}
<table>
<thead>
<tr>
  <th align="center">üíª</th>
  <th></th>
  <th align="center">üåè</th>
</tr>
</thead>
<tbody>
<tr>
  <td><span data-qmd="MS in **Computer Science**, Stanford University"></span></td>
  <td>&rarr;</td>
  <td><span data-qmd="PhD in **International Relations**, Columbia University"></span></td>
</tr>
<tr>
  <td><span data-qmd="Assistant Teaching Professor, **Data Science and Analytics**"></span></td>
  <td>&rarr;</td>
  <td><span data-qmd="Courtesy Teaching Professor, **McCourt School of Public Policy**"></span></td>
</tr>
</tbody>
</table>
```

* Cross-listed course: [DSAN 5450: Data Ethics and Policy](https://jjacobs.me/dsan5450/){target='_blank'}
  * All materials online + open-source, available at that link!

## Dissertation: "Our Word is Our Weapon" {.smaller .crunch-img .title-12}

* Cold War arms shipments (SIPRI) vs. propaganda (*–ü–µ—á–∞—Ç—å –°–°–°–†*): here, to üá™üáπ

![](images/eth_arms.svg){fig-align="center"}

* I only know the most basic <mark data-term="—É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ —Å–ª–æ–≤–∞">words</mark> üôà but I will try my best!

## Data Ethics in Times of War {.smaller .crunch-title .crunch-img .crunch-th .crunch-td-last}

```{=html}
<table>
<colgroup>
  <col span="1" style="width: 50%;">
  <col span="1" style="width: 50%;">
</colgroup>
<thead>
<tr>
  <th align="center">üáµüá∏</th>
  <th align="center">üá∫üá¶</th>
</tr>
</thead>
<tbody>
<tr>
  <td style="border-bottom: 0px !important;"><span data-qmd="2015-2023: Taught **Mobile App Development** in **West Bank**+**Gaza**"></span></td>
  <td style="border-bottom: 0px !important;"><span data-qmd="So when I learned of **–¥—ñ—è**, I thought some lessons/skills could apply!"></span></td>
</tr>
<tr>
  <td align="center" style="vertical-align: top;"><img src='images/doroob_crop.png' width="80%"></td>
  <td align="center" style="vertical-align: top;"><img src='images/diia.png' width="78%"></td>
</tr>
</tbody>
</table>
```

# Ethical Frameworks: Implicit and Explicit {data-stack-name="Ethical Frameworks"}

* We are always using <mark data-term="–µ—Ç–∏—á–Ω—ñ —Ç–µ–æ—Ä—ñ—ó">**ethical frameworks**</mark>, even when we don't realize it!

::: {.notes}

–ï—Ç–∏—á–Ω—ñ —Ç–µ–æ—Ä—ñ—ó = Etychni teoriyi = Ethical theories

:::

## Cake Cutting

* You and a friend are very <mark data-term="–≥–æ–ª–æ–¥–Ω—ñ.">hungry</mark>, and you come across a <mark data-term="—Å–º–∞—á–Ω–∏–π —Ç–æ—Ä—Ç">tasty **cake**</mark>!
* How do you divide it?

## Ethical Frameworks

* Your instinct may be: divide <mark data-term="–ø–æ—Ä—ñ–≤–Ω—É">**equally**</mark>, so that you and your friend are treated equally
* But... are you treated equally?
* What if your friend is 10 times **hungrier**?
* What if your stomach is sensitive to sugar, so you can't eat more than $1/4$?

::: {.notes}

–ø–æ—Ä—ñ–≤–Ω—É = porivnu = equally

:::

## Math to the Rescue... For Now!

For $N = 2$, simple **mechanism** for **envy-free** division:

1. –ê–Ω–∞—Å—Ç–∞—Å—ñ—è cuts the cake into **2 pieces** however she wants
2. –í–µ—Ä–æ–Ω—ñ–∫–∞ chooses which piece to eat
3. ...–î–∞–≤–∞–π —Ä–æ–∑–ø–æ—á–Ω–µ–º–æ —Ü—é –≤–µ—á—ñ—Ä–∫—É!^[If this is wrong... it's [this guy](https://www.tiktok.com/@learnukrainianwithme/video/7328868603602668842){target='_blank'}'s fault]

::: {.notes}

Davay rozpochnemo tsyu vechirku = let's get this party started!

:::

## Ethical Complexity: 3 People {.crunch-title}

::: {.columns}
::: {.column width="50%"}

![](images/cake_cutting_left.jpeg){fig-align="center"}

:::
::: {.column width="50%"}

![](images/cake_cutting_right.jpeg){fig-align="center"}

:::
:::

::: {.aside}

*Quanta* Magazine, 6 Oct 2016, ["How to Cut Cake Fairly and Finally Eat It Too"](https://www.quantamagazine.org/new-algorithm-solves-cake-cutting-problem-20161006/){target='_blank'}

:::

## Ethical Complexity: $n$ People {.crunch-title .crunch-blockquote .crunch-p .crunch-img}

::: {.text-90}

> We consider the well-studied cake cutting problem in which the goal is to find an envy-free allocation based on queries from $n$ agents. The problem has received attention in computer science, mathematics,
and economics. It has been a major open problem whether there exists a discrete bounded envy-free
protocol. We resolve the problem with a discrete bounded envy-free protocol for any number of agents. The maximum number of queries required by the protocol is $n^{n^{n^{n^{n^n}}}}$. [@aziz_discrete_2016]

:::

::: {.columns}
::: {.column width="50%"}

![](images/cake_cutting_google.jpeg){fig-align="center"}

:::
::: {.column width="50%"}

![](images/cake_cutting_google_uk.jpeg){fig-align="center"}

:::
:::

::: {.notes}

nevyznachenyy (nevoyznachenyy) = unspecified

:::

## Ethical Framework Implicit in Learning "Who You Are"! {.smaller .crunch-title .crunch-quarto-figure}

::: {.columns}
::: {.column width="50%"}

```{python}
#| label: orig-wordcloud
#| fig-align: center
import spacy
nlp = {
  'en': spacy.load("en_core_web_sm"),
  'uk': spacy.load("uk_core_news_sm")
}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
portfolio_df = pd.read_csv("Student_portfolios.csv")
portfolio_df = portfolio_df[~pd.isna(portfolio_df['Bio'])].copy()
bio_list_en = list(portfolio_df['Bio'].values)
# Preprocess bios individually
def clean_bio(raw_bio_str, lang):
  lang_nlp = nlp[lang]
  bio_doc = lang_nlp(raw_bio_str)
  bio_tokens = [t.text for t in bio_doc if not t.is_stop]
  bio_sent = " ".join(bio_tokens)
  return bio_sent
def clean_bios(raw_bio_list, lang):
  cleaned_sents = []
  for cur_bio in raw_bio_list:
    bio_sent = clean_bio(cur_bio, lang)
    cleaned_sents.append(bio_sent)
  return cleaned_sents
cleaned_bio_list_en = clean_bios(bio_list_en, 'en')
def combine_bios(bio_list):
  bio_str = "\n".join(bio_list)
  return bio_str
bio_str_en = combine_bios(cleaned_bio_list_en)
wc_options = {
  'background_color': 'white',
  'font_path': 'assets/Roboto.ttf',
  'scale': 8,
  'random_state': 2024,
  'width': 500,
  'height': 400,
}
def display_wordcloud(text_str):
  cloud = WordCloud(**wc_options)
  cloud.generate(text_str)
  plt.imshow(cloud, interpolation='bilinear')
  plt.axis("off")
display_wordcloud(bio_str_en)
```

:::
::: {.column width="50%"}

```{python}
#| label: orig-wordcloud-uk
#| fig-align: center
bio_list_uk = list(portfolio_df['Bio_uk'].values)
cleaned_bio_list_uk = clean_bios(bio_list_uk, 'uk')
bio_str_uk = combine_bios(cleaned_bio_list_uk)
display_wordcloud(bio_str_uk)
```

:::
:::

* How might this be <mark data-term='–Ω–µ—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∏–π'>biased</mark>, if we want <mark data-term='—Ä—ñ–≤–Ω–∞ —á–∞—Å—Ç–∫–∞'>equal contribution</mark>?

## Now With Fake Jeff Bio {.crunch-title .crunch-img}

::: {.columns}
::: {.column width="50%"}

```{python}
#| label: jeff-bio-en
jj_sent_en = "Jeff teaches Data Ethics at Georgetown University."
jj_bio_en = " ".join([jj_sent_en] * 100)
print(jj_bio_en[:230] + "...")
```

```{python}
#| label: wordcloud-jeff-bio-en
#| fig-align: center
bio_list_fake_en = cleaned_bio_list_en.copy()
cleaned_jj_bio_en = clean_bio(jj_bio_en, 'en')
bio_list_fake_en.append(cleaned_jj_bio_en)
bio_str_fake_en = combine_bios(bio_list_fake_en)
bio_cloud_fake_en = WordCloud(**wc_options)
bio_cloud_fake_en.generate(bio_str_fake_en)
plt.imshow(bio_cloud_fake_en, interpolation='bilinear')
plt.axis("off")
```

:::
::: {.column width="50%"}

```{python}
#| label: jeff-bio-uk
jj_sent_uk = "–î–∂–µ—Ñ –≤–∏–∫–ª–∞–¥–∞—î –µ—Ç–∏–∫—É –¥–∞–Ω–∏—Ö —É –î–∂–æ—Ä–¥–∂—Ç–∞—É–Ω—Å—å–∫–æ–º—É —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—ñ."
jj_bio_uk = " ".join([jj_sent_uk] * 100)
print(jj_bio_uk[:200] + "...")
```

```{python}
#| label: wordcloud-jeff-bio-uk
#| fig-align: center
bio_list_fake_uk = cleaned_bio_list_uk.copy()
cleaned_jj_bio_uk = clean_bio(jj_bio_uk, 'uk')
bio_list_fake_uk.append(cleaned_jj_bio_uk)
bio_str_fake_uk = combine_bios(bio_list_fake_uk)
bio_cloud_fake_uk = WordCloud(**wc_options)
bio_cloud_fake_uk.generate(bio_str_fake_uk)
plt.imshow(bio_cloud_fake_uk, interpolation='bilinear')
plt.axis("off")
```

:::
:::

## Normalizing Counts {.crunch-title .crunch-img}

* This is where **ethical framework** comes into play, whether implicit or explicit!
* Normalized: each person $i$ gets weight $\omega_i = 1/N$

::: {.columns}
::: {.column width="50%"}

```{python}
#| label: normalize-freqs-en
# Each bio's freqs should count 1/N in overall
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import normalize
cv_en = CountVectorizer(
  min_df=1,
  # stop_words='english'
)
dtm_en_sparse = cv_en.fit_transform(bio_list_fake_en)
dtm_en = dtm_en_sparse.todense()
#print(dtm.shape)
row_sums_en = dtm_en.sum(axis=1)
#print(row_sums.shape)
dtm_r0_en = dtm_en[0,:]
r0_sum_en = dtm_r0_en.sum()
r0_norm_en = dtm_r0_en / r0_sum_en
# Now use `normalize()`
dtm_norm_en = normalize(np.asarray(dtm_en), axis=1, norm='l1')
# And compute col sums of normalized DTM
dtm_colsums_en = dtm_norm_en.sum(axis=0)
# And combine with the words in a df
freq_df_en = pd.DataFrame({
  'term': cv_en.get_feature_names_out(),
  'freq': dtm_colsums_en
})
#freq_df['term'].values
```

```{python}
#| label: normalized-wordcloud-en
#| fig-align: center
freq_dict_en = freq_df_en.set_index('term').to_dict(orient='dict')['freq']
bio_cloud_eq_en = WordCloud(**wc_options)
bio_cloud_eq_en.generate_from_frequencies(freq_dict_en)
plt.imshow(bio_cloud_eq_en, interpolation='bilinear')
plt.axis("off")
```

:::
::: {.column width="50%"}

```{python}
#| label: normalize-freqs-uk
cv_uk = CountVectorizer(
  min_df=1,
)
dtm_uk_sparse = cv_uk.fit_transform(bio_list_fake_uk)
dtm_uk = dtm_uk_sparse.todense()
#print(dtm.shape)
row_sums_uk = dtm_uk.sum(axis=1)
#print(row_sums.shape)
dtm_r0_uk = dtm_uk[0,:]
r0_sum_uk = dtm_r0_uk.sum()
r0_norm_uk = dtm_r0_uk / r0_sum_uk
# Now use `normalize()`
dtm_norm_uk = normalize(np.asarray(dtm_uk), axis=1, norm='l1')
# And compute col sums of normalized DTM
dtm_colsums_uk = dtm_norm_uk.sum(axis=0)
# And combine with the words in a df
freq_df_uk = pd.DataFrame({
  'term': cv_uk.get_feature_names_out(),
  'freq': dtm_colsums_uk
})
#freq_df['term'].values
```

```{python}
#| label: normalized-wordcloud-uk
#| fig-align: center
freq_dict_uk = freq_df_uk.set_index('term').to_dict(orient='dict')['freq']
bio_cloud_eq_uk = WordCloud(**wc_options)
bio_cloud_eq_uk.generate_from_frequencies(freq_dict_uk)
plt.imshow(bio_cloud_eq_uk, interpolation='bilinear')
plt.axis("off")
```

:::
:::

## <mark data-term="–†—ñ–≤–Ω—ñ—Å—Ç—å –ß–æ–≥–æ?">Equality of What?</mark> {.crunch-title .crunch-ul .math-80 .title-09}

* Is $\omega_i = 1/N$ the "correct" choice?
* Should we ensure **each person/ministry gets $\geq 1$ big word**
* Maybe it should be 50% **public sector**, 50% **private sector**
* **Higher $\omega$** for those **sitting closer to me**: $\omega_i = \frac{1}{\text{dist}(\text{Jeff},i)}$
* ...How to decide? **Objective Function** $f$ + **Constraints** $g$!

::: {.columns}
::: {.column width="50%"}

<center style="font-size: 80%;">
In General:
</center>

$$
\begin{align*}
\max_{c \mkern1.0mu \in \mkern1.0mu \text{Choices}} \; & f(c) = \text{Goodness of }c \\
\text{subject to } & g(c) = \text{Constraints on }c
\end{align*}
$$

:::
::: {.column width="50%"}

<center style="font-size: 80%;">
Our Example:
</center>

$$
\begin{align*}
\max_{\omega \mkern1.0mu \in \mkern1.0mu [0,1]^N} \; & f(\omega) = \textstyle\sum_{i=0}^{N}\omega_i u_i \\
\text{subject to } &\textstyle\sum_{i=0}^{N}\omega_i = 1
\end{align*}
$$

:::
:::

::: {.notes}

–†—ñ–≤–Ω—ñ—Å—Ç—å —á–æ–≥–æ? = Rivnist' choho?

:::

# Resource Allocation {data-stack-name="Resource Allocation"}

## Background: Indochina Mine Defusal {.crunch-title}

::: {.columns}
::: {.column width="50%"}

* Using **GIS** to allocate **mine defusal teams** to regions of Indochina mined by the US
* What information do we need about each region?
  * Absolute population? Population density? Mines per person?
  * Can equipment be transported there?

:::
::: {.column width="50%"}

![](images/indochina.png){fig-align="center" width="90%"}

:::
:::

## Incremental Model

* As a <mark data-term="–Ω–∞–π–ø—Ä–æ—Å—Ç—ñ—à–∏–π">Minimum Viable Product (MVP)</mark>, first just consider the **non-constrained** optimization
* Add in constraints incrementally---as you add, evaluate **feasibility** relative to **goals** (for example, how close can you get to MVP outcome?)
* Iterative process $\implies$ <mark data-term="–≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è">visualization</mark> can be crucially helpful!

::: {.notes}

vizualizatsiya

:::

## Sidebar: –£–∫—Ä–∞—ó–Ω–∞ in ArcGIS

* [Regions of –£–∫—Ä–∞—ó–Ω–∞ on Georgetown ArcGIS server](https://georgetownuniv.maps.arcgis.com/apps/mapviewer/index.html?webmap=50668b6cb4664b79948817100400b0dc){target='_blank'}
* Full-on GIS can take several weeks to get used to, so we'll use a simpler approach here (using **ObservableJS**)
* But ArcGIS with Ukraine maps will be used for student assignment(s) in [DSAN 6750 / PPOL 6805: **Geographic Information Systems**](https://jjacobs.me/dsan6750){target='_blank'} (Fall 2024)
  * All materials online and open-source, if you want to follow along!

# Interactive Examples!

* [ObservableJS Interactive Dashboard](https://observablehq.com/@jpowerj/resource-allocation-ukraine){target='_blank'}

## Problem 1: Divisibility {.crunch-title}

::: {.columns}
::: {.column width="50%"}

* Can we <mark data-term="—Ä–æ–∑–¥—ñ–ª—è—Ç–∏">split</mark> our resource infinitely?
* $1/2$ of a gas mask doesn't help us! ‚ò†Ô∏è

:::
::: {.column width="50%"}

![](images/tennis_lifehack.jpeg){fig-align="center"}

:::
:::

::: {.notes}

rozdilyaty

:::

## Problem 2: Distance Metrics {.crunch-title}

::: {.columns}
::: {.column width="50%"}

* Euclidean / "Straight line" distance **not always the "right" measure!**
* [Allocation Challenge 2: Distance-Based Decay](https://observablehq.com/@jpowerj/resource-allocation-distances)

:::
::: {.column width="50%"}

![From @shahid_comparison_2009](images/medical_distances.jpg){fig-align="center" width="75%"}

:::
:::

## Problem 3: Capabilities {.crunch-title .crunch-ul .crunch-img .smaller}

::: {.columns}
::: {.column width="60%"}

* How effectively can people **convert** <mark data-term="—Ä–µ—Å—É—Ä—Å–∏">resources</mark> $\leadsto$ <mark data-term="–ö–æ—Ä–∏—Å–Ω—ñ—Å—Ç—å">utility</mark>?
* Key example: <mark data-term="—Ö–∞—Ä—á—É–≤–∞–Ω–Ω—è">nutrition</mark> during <mark data-term="–≤–∞–≥—ñ—Ç–Ω—ñ—Å—Ç—å">pregnancy</mark>
* [Johns Hopkins Medicine](https://www.hopkinsmedicine.org/health/wellness-and-prevention/nutrition-during-pregnancy){target='_blank'}: **300** <mark data-term="–∫–∞–ª–æ—Ä—ñ–π –±—ñ–ª—å—à–µ">more calories</mark> per day

![[Oxfam International, 3 Apr 2024](https://www.oxfam.org/en/press-releases/people-northern-gaza-forced-survive-245-calories-day-less-can-beans-oxfam){target='_blank'}](images/gaza_calories.jpeg){fig-align="center"}

:::
::: {.column width="40%"}

![@sen_commodities_1985](images/sen_capabilities.jpg){fig-align="center" width="75%"}

:::
:::

# Privacy-Preserving Computation {data-stack-name="Privacy"}

## Disclosure Avoidance {.crunch-title .crunch-ul .crunch-p}

* 1990, 2000, and 2010 Census: "Data Swapping":

> We inject "noise" into the data by swapping records for certain households with those from households with similar characteristics in a nearby area. The Census does not release information about its specific methods for swapping. While this <mark data-term="–∫–æ–Ω—Ñ—ñ–¥–µ–Ω—Ü—ñ–π–Ω—ñ—Å—Ç—å">confidentiality</mark> around swapping techniques is important to protect against disclosure, it means that the practice is <mark data-term="–Ω–µ –ø—Ä–æ–∑–æ—Ä–∏–π">not transparent</mark> to data users

::: {.notes}

konfidentsiynist π, ne prozoryy

:::

## The Death of Data Swapping

* But... –®—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç, –í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ, etc... ü§ñ

> If traditional disclosure avoidance techniques were applied to the 2020 Census data, the **amount of noise required to protect** against **new attacks** would make census data **unfit** for most uses

$\implies$ We need a framework for **quantifying** "acceptable" privacy loss (tradeoff!)

::: {.notes}

AI = Shtuchnyy intelekt

:::

## Differential Privacy

* Implemented for the [**2020 US Census**](https://www2.census.gov/library/publications/decennial/2020/2020-census-disclosure-avoidance-handbook.pdf){target='_blank'}
* The key idea: as people make more and more **queries** to your DB, your **response** gets less and less accurate

![Figure 8.1: "The Privacy-Loss Budget (Epsilon) Acts as a Dial on the Level of Noise", from US Census Bureau (2021), [*Disclosure Avoidance for the 2020 Census: An Introduction*](https://www2.census.gov/library/publications/decennial/2020/2020-census-disclosure-avoidance-handbook.pdf){target='_blank'}](images/census_epsilon_crop.svg){fig-align="center"}

## Why Can't We Just Restrict Queries to Aggregated Statistics?

*The answer:* **Differencing Attacks** üò∞

* Query 1: Number of –Ω–∞—Ä–æ–¥–Ω–∏—Ö –¥–µ–ø—É—Ç–∞—Ç—ñ–≤ in –í–µ—Ä—Ö–æ–≤–Ω–∞ –†–∞–¥–∞ with <mark data-term="–∑–∞—Ö–≤–æ—Ä—é–≤–∞–Ω–Ω—è">disease</mark> $X$
* Query 2: Number of –Ω–∞—Ä–æ–¥–Ω–∏—Ö –¥–µ–ø—É—Ç–∞—Ç—ñ–≤ in –í–µ—Ä—Ö–æ–≤–Ω–∞ –†–∞–¥–∞, except –ì–æ–ª–æ–≤–∞, with <mark data-term="–∑–∞—Ö–≤–æ—Ä—é–≤–∞–Ω–Ω—è">disease</mark> $X$

## Real Data: —Å–µ–ª–∏—â–µ –ü–æ–Ω–æ—Ä–Ω–∏—Ü—è

```{=html}
<div id='ponornycka' style="height: 400px;"></div>
<script src="assets/ponornycka.js"></script>
```

## Example with Real Data {.smaller}

```{=html}
<style>
/* Table section sep */
.tss td:nth-child(1), .tss td:nth-child(4), .tss td:nth-child(7), .tss td:nth-child(10) {
  /* background-color: green; */
  border-right: 2px solid darkgrey;
}
</style>
<table class='text-90 tss'>
<thead>
<tr>
  <th rowspan="2" align="center" style="vertical-align: middle;" class="trb">–≥—Ä–æ–º–∞–¥–∞</th>
  <th colspan="3" align="center" class="trb">True Population</th>
  <th colspan="3" align="center" class="trb">Noise</th>
  <th colspan="3" align="center" class="trb">Noisy Counts</th>
  <th colspan="3" align="center">Post-Processed</th>
</tr>
<tr>
  <th>&lt;18</th>
  <th>18+</th>
  <th class="trb">Total</th>
  <th>&lt;18</th>
  <th>18+</th>
  <th class="trb">Total</th>
  <th>&lt;18</th>
  <th>18+</th>
  <th class="trb">Total</th>
  <th>&lt;18</th>
  <th>18+</th>
  <th>Total</th>
</tr>
</thead>
<tbody>
<tr>
  <td>–ó–µ–ª–µ–Ω–∞ –ü–æ–ª—è–Ω–∞</td>
  <td align="right">20</td>
  <td align="right">57</td>
  <td align="right">77</td>
  <td align='right'><span class='pm-neutral'>0</span></td>
  <td align='right'><span class='pm-subtract'>-4</span></td>
  <td align='right'><span class='pm-add'>+2</span></td>
  <td align='right'>20</td>
  <td align='right'>53</td>
  <td align='right'>(&ne;)79</td>
  <td align='right'>23 (+3)</td>
  <td align='right'>54 (+1)</td>
  <td align='right'>77 (-2)</td>
</tr>
<tr>
  <td>–†–∏—Ö–ª–∏</td>
  <td>30</td>
  <td>82</td>
  <td>112</td>
  <td><span class='pm-subtract'>-3</span></td>
  <td><span class='pm-add'>+2</span></td>
  <td><span class='pm-add'>+3</span></td>
  <td>27</td>
  <td>84</td>
  <td>115</td>
  <td>28 (+1)</td>
  <td>85 (+1)</td>
  <td>113 (-2)</td>
</tr>
<tr>
  <td>–í–µ–ª–∏–∫–∏–π –õ—ñ—Å</td>
  <td>20</td>
  <td>86</td>
  <td>106</td>
  <td><span class='pm-subtract'>-2</span></td>
  <td><span class='pm-add'>+1</span></td>
  <td><span class='pm-add'>+1</span></td>
  <td>18</td>
  <td>87</td>
  <td>107</td>
  <td>17 (-1)</td>
  <td>87</td>
  <td>104 (-3)</td>
</tr>
<tr>
  <td>–†–æ–∑–ª—å–æ—Ç–∏</td>
  <td>150</td>
  <td>155</td>
  <td>305</td>
  <td><span class='pm-neutral'>0</span></td>
  <td><span class='pm-add'>+2</span></td>
  <td><span class='pm-neutral'>0</span></td>
  <td>150</td>
  <td>157</td>
  <td>305</td>
  <td>150</td>
  <td>156 (-1)</td>
  <td>306 (+1)</td>
</tr>
<tr>
  <td align="right"><b>Total</b></td>
  <td>220</td>
  <td>380</td>
  <td>600</td>
  <td><span class='pm-subtract'>-5</span></td>
  <td><span class='pm-add'>+1</span></td>
  <td><span class='pm-add'>+6</span></td>
  <td>215</td>
  <td>381</td>
  <td>606</td>
  <td>218</td>
  <td>382</td>
  <td>600</td>
</tr>
</tbody>
</table>
```

## The Magic (Noise!) {.crunch-title .crunch-ul .crunch-math .crunch-p .math-80}

* If noise values $\nu$ drawn i.i.d. from **Laplace distribution** (pdf):

$$
\text{Lap}(x; \varepsilon) = \frac{1}{2\varepsilon}\exp\mkern-2.5mu\left[-\frac{|x|}{\varepsilon}\right]
$$

* Can **mathematically guarantee** $\varepsilon$-differential privacy!

$$
\Pr\left[M(D) \in S\right] \leq e^\varepsilon \times \Pr\mkern-2.5mu\left[M(D') \in S\right]
$$

* Can **quantify** amount of privacy loss $\mathcal{L}(\gamma)$ from observing $\gamma$

$$
\mathcal{L}(\gamma) = \ln\left[\frac{\Pr[M(D) = \gamma]}{\Pr[M (D') = \gamma]}\right]
$$

## The Metaphor

![Seurat, *A Sunday Afternoon on the Island of La Grande Jatte*, [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:A_Sunday_on_La_Grande_Jatte,_Georges_Seurat,_1884.jpg){target='_blank'}](images/seurat.jpg){fig-align="center"}

## References / Resources

::: {#refs}
:::

<!-- * Privacy: [Big Data and the Public Good](https://www.cambridge.org/core/books/privacy-big-data-and-the-public-good/1ACB10292B07EC30F071B4AD9650955C){target='_blank'} -->

# Q & A

# Thank You!

# Appendix Slides

## Sensitivity {.smaller}

> Consider a real-valued function $f$. The (worst-case, or global) sensitivity of $f$ is the maximum absolute value by which the addition or deletion of a single database row can change the value of $f$:

$$
\Delta f = \max_{D, D'}|f(D) - f(D')|
$$

> Queries of the form "How many people in the database are over six feet tall?‚Äù have sensitivity $\Delta f = 1$, since the presence or absence of any individual in $D$ can affect the true answer by at most 1. Thus, the Laplace mechanism returns the true count perturbed by a random draw from $\text{Lap}(x; 1/\varepsilon)$. [@dwork_differential_2014]